{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["LuDYBSu0WbwN","qpROeSDbc-jp","SQsHJPRZ-ZGn"],"authorship_tag":"ABX9TyPNKSfohotPxL8RsVKO3Myq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MODEL ONE"],"metadata":{"id":"LuDYBSu0WbwN"}},{"cell_type":"code","source":["# pip install networkx==2.3\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","import pandas as pd\n","import re\n","import numpy as np\n","from collections import Counter\n","import seaborn as sns\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk \n","import string\n","plt.style.use('ggplot')\n","pd.set_option('display.max_colwidth', 100)\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import nltk\n","\n","import tweepy\n","import csv\n","import sys\n","from importlib import reload\n","reload(sys)\n","\n","#nltk.download('stopwords')\n","ps = nltk.PorterStemmer()\n","string.punctuation\n","wn = nltk.WordNetLemmatizer()\n","\n","from nltk.corpus import stopwords\n","stopwords = stopwords.words('english')\n","#print(stop_words)\n","\n","#from nltk.corpus import stopwords\n","#stopword = nltk.corpus.stopwords.words('english')\n","\n","def remove_punct(text):\n","    text  = \"\".join([char for char in text if char not in string.punctuation])\n","    text = re.sub('[0-9]+', '', text)\n","    return text\n","\n","def tokenization(text):\n","    text = re.split('\\W+', text)\n","    return text\n","\n","def remove_stopwords(text):\n","    text = [word for word in text if word not in stopwords]\n","    return text\n","\n","def stemming(text):\n","    text = [ps.stem(word) for word in text]\n","    return text\n","def lemmatizer(text):\n","    text = [wn.lemmatize(word) for word in text]\n","    return text\n","\n","import re\n","def untokenize(words):\n","    \"\"\"\n","    Untokenizing a text undoes the tokenizing operation, restoring\n","    punctuation and spaces to the places that people expect them to be.\n","    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n","    except for line breaks.\n","    \"\"\"\n","    text = ' '.join(words)\n","    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n","    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n","    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n","    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n","    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n","         \"can not\", \"cannot\")\n","    step6 = step5.replace(\" ` \", \" '\")\n","    return step6.strip()\n","\n","# Mandatory attributes# Age of the account# Number of followers# Number of friends# Verified Status# Number of tweets\n","# Optional Attributes# Location# Number of lists# Has a bio\n","\n","def calculateAgeScore(age):\n","  if age <= 0:\n","    return 0\n","  elif age > 0 and age <= 6:\n","    return 1\n","  elif age > 6 and age <= 24:\n","    return 2\n","  elif age > 24 and age <= 60:\n","    return 3\n","  elif age > 60 and age <= 120:\n","    return 4\n","  elif age > 120:\n","    return 5\n","  else :\n","    return 0\n","\n","\n","def calculateFollowerRatioScore(row):\n"," if row['friends_count'] == 0:\n","  return 0\n","\n"," ratio = row['followers_count'] / row['friends_count']\n","\n"," if ratio < 0.5:\n","  return 0\n"," elif ratio >= 0.5 and ratio <= 1:\n","  return 1\n"," elif ratio > 1 and ratio <= 2:\n","  return 2\n"," elif ratio > 2 and ratio <= 5:\n","  return 3\n"," elif ratio > 5 and ratio <= 10:\n","  return 4\n"," elif ratio > 10:\n","  return 5\n"," else :\n","  return 0\n","\n","\n","def calculateVerifiedStatusScore(isVerified):\n"," if isVerified:\n","    return 5\n"," else :\n","    return 0\n","\n","def calculateLocationScore(location):\n"," if pd.isnull(location):\n","  return 0\n"," elif \"sri lanka\" in location.lower() or \"srilanka\" in location.lower() or \"colombo\" in location.lower():\n","  return 5\n"," elif pd.isnull(location) is False:\n","  return 4\n","\n","def calculateUserListsScore(lists):\n"," if lists < 2:\n","  return 0\n"," elif lists >= 2 and lists <= 10:\n","  return 1\n"," elif lists > 10 and lists <= 30:\n","  return 2\n"," elif lists > 30 and lists <= 60:\n","  return 3\n"," elif lists > 60 and lists <= 90:\n","  return 4\n"," elif lists > 90:\n","  return 5\n","\n","def calculateUserBioScore(bio):\n"," if pd.isnull(bio):\n","  return 0\n"," else :\n","  return 5\n","\n","def getNormalizedTrustScore(x):\n"," minX = 0\n"," maxX = 7.5\n"," return (x - minX) / maxX - minX\n","import datetime\n","from dateutil.parser import parse\n","from datetime import datetime\n","\n","def getAgeFromCreatedDate(date_str):\n","\n"," #start_date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n"," start_date=date_str\n"," end_date = datetime(2019, 5, 20, 00, 00, 00)\n"," num_of_days=end_date -parse(start_date)\n"," num_months=num_of_days/30 \n"," #age=int(num_months)\n"," #num_months = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month)\n"," #age= (num_months / np.timedelta64(1, 'D')).astype(int)\n"," return num_months\n","\n","\n","def calculateTweetCountScore(tweets):\n","  if tweets <= 10:\n","   return 0\n","  elif tweets > 10 and tweets <= 100:\n","   return 1\n","  elif tweets > 100 and tweets <= 1000:\n","   return 2\n","  elif tweets > 1000 and tweets <= 5000:\n","   return 3\n","  elif tweets > 5000 and tweets <= 10000:\n","   return 4\n","  elif tweets > 10000:\n","   return 5\n","  else :\n","   return 0\n","\n","\n","def getNormalizedTrustScore(x):\n"," minX = 0\n"," maxX = 7.5\n"," return (x - minX) / maxX - minX\n","\n","\n","def model1(csv_path):\n","    ordinary_tweets_data = pd.read_csv(csv_path)\n","    ordinary_tweets_data.columns = ordinary_tweets_data.columns.str.replace(' ','_')\n","    pattern='Colombo,Sri Lanka |Colombo, Sri Lanka|Sri Lanka| Srilanka ðŸ‡±ðŸ‡° |Colombo|Colombo, Srilanka|Colombo Sri Lanka|Sri Lankan'\n","    mask=ordinary_tweets_data['Tweet_Location'].str.contains(pattern,case=False,na=False)\n","    ordinary_tweets_data_mask=ordinary_tweets_data[mask]\n","    df  = pd.DataFrame(ordinary_tweets_data_mask[['Tweet_Id','Tweet_Type','User_Id','Name','Screen_Name', 'Tweet_Content','Retweets_Received','Favourites_Received',\n","                            'Verified_or_Non-Verified','User_Followers','User_Following','Favourites_Count','Statuses_Count',\n","                             'User_Account_Creation_Date','User_Bio','Tweet_Location','Tweet_Posted_Time_(UTC)','Protected_or_Non-protected']])\n","    df['User_Id_punct'] = df['User_Id'].apply(lambda x: x.replace('\"', ''))\n","    df['Tweet_Id_punct'] = df['Tweet_Id'].apply(lambda x: x.replace('\"', ''))\n","    pattern='Colombo,Sri Lanka |Colombo, Sri Lanka|Sri Lanka| Srilanka ðŸ‡±ðŸ‡° |Colombo|Colombo, Srilanka|Colombo Sri Lanka|Sri Lankan'\n","    mask=df['Tweet_Location'].str.contains(pattern,case=False,na=False)\n","    tweet_df=df[mask]\n","    model_1_dataset3=tweet_df.drop_duplicates(subset=['Screen_Name'])\n","\n","    model_1_result=model_1_dataset3[['User_Id', 'Name','Screen_Name']].copy()\n","    model_1_result['User_Id_punct'] = model_1_result['User_Id'].apply(lambda x: x.replace('\"', ''))\n","    model_1_dataset2=model_1_result[['User_Id_punct', 'Name','Screen_Name']].copy()\n","    model_1_dataset2.rename(columns = {'User_Id_punct':'User_Id'}, inplace = True)\n","    #### input your credentials here\n","    consumer_key = 'CQldZWCZjOPCmcuggVEYDLKb8'\n","    consumer_secret = 'rVGwBrR5PKxZs2yIpwPIbLP6TzhARo7C4cRodrCKVFSEGTOOWi'\n","    access_token = '1506450127057305604-XjukkZt3QPyCQ8oZTRC4qpQnglQu4u'\n","    access_token_secret = 'xY7v8pWdKZaQcGGVokNQIwgwoiMSlmZlbn6nWAaytJxN2'\n","    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","    auth.set_access_token(access_token, access_token_secret)\n","    api = tweepy.API(auth, wait_on_rate_limit = True)\n","    import json\n","    col_list = model_1_dataset2['Screen_Name'].tolist()\n","    col_list.remove('dula_aluthge')\n","    col_list.remove('SukyT91')\n","    col_list.remove('cpaolag')\n","    list = []\n","    for item in col_list:\n","        #x='\"{}\"'.format(item)\n","        users = api.lookup_users(screen_name =item)\n","        \n","\n","        \n","        for user in users:\n","      #text = user._json[\"full_text\"]\n","    \n","          refined_tweet = {'screen_name' : user.screen_name,\n","                      'User_Id' : user.id,\n","                      'Name':user.name,\n","                      'followers_count' : user.followers_count,\n","                      'friends_count' : user.friends_count,\n","                      'created_at' : user.created_at,\n","                      'statuses_count' : user.statuses_count,\n","                      'listed_count' : user.listed_count, \n","                      'location' : user.location,\n","                      'verified' : user.verified,\n","                      'User_Bio':user.description}\n","      \n","          list.append(refined_tweet)\n","\n","\n","    ordinary_tweets_users = pd.DataFrame(list)\n","    model1_data=ordinary_tweets_users.copy()\n","    model1_data['created_at']=model1_data['created_at'].astype(str).str[:30]   \n","    model_1_result=model1_data[['User_Id', 'Name']].copy()\n","\n","    # Mandatory attributes# Age of the account# Number of followers# Number of friends# Verified Status# Number of tweets\n","    # Optional Attributes# Location# Number of lists# Has a bio\n","\n","    # Mandatory attributes#\n","    model_1_result['verifiedStatus'] = model1_data['verified'].apply(calculateVerifiedStatusScore)\n","\n","    model_1_result['getAgeFromCreatedDate'] = model1_data['created_at'].apply(getAgeFromCreatedDate)\n","    model_1_result['getAgeFromCreatedDate'] = pd.to_numeric(model_1_result['getAgeFromCreatedDate'].dt.days, downcast='integer')\n","    model_1_result['calculateAgeScore'] = model_1_result['getAgeFromCreatedDate'].apply(calculateAgeScore)\n","\n","    model_1_result['followerRatioScore'] =model1_data.apply(lambda row:calculateFollowerRatioScore(row), axis=1)\n","\n","    model_1_result['tweetCountScore'] =  model1_data['statuses_count'].apply(calculateTweetCountScore)\n","\n","\n","\n","    #Optional Attributes#\n","    model_1_result['User_Bio'] = model1_data['User_Bio'].apply(calculateUserBioScore)\n","    model_1_result['user_location'] = model1_data['location'].apply(calculateLocationScore)\n","    model_1_result['listCountScore'] = model1_data['listed_count'].apply(calculateUserListsScore)  \n","\n","    weighted_mandatory_trust_score = 0\n","    mandatory_attribute_weight = 1\n","    weighted_mandatory_trust_score = 0\n","    mandatory_attribute_weight = 1\n","    model_1_result= model_1_result.eval('weighted_optional_trust_score =0+(User_Bio *0.5 +user_location*0.5+listCountScore*0.5)/3')\n","    model_1_result= model_1_result.eval('weighted_mandatory_trust_score =0+(verifiedStatus *1 +calculateAgeScore*1+followerRatioScore*1+tweetCountScore*1)/4')\n","    model_1_result=model_1_result.eval('final_weighted_score=0+weighted_optional_trust_score+weighted_mandatory_trust_score')\n","\n","    ## convert to 0-1 value\n","    model_1_result['normalized_final_weighted_score'] = model_1_result['final_weighted_score'].apply(getNormalizedTrustScore)\n","    model_1_result1 =model_1_result[['User_Id','Name','weighted_mandatory_trust_score','weighted_optional_trust_score','final_weighted_score']].copy()\n","    model_1_result2 =model_1_result[['User_Id','Name','normalized_final_weighted_score']].copy()\n","    ##calling to model 2\n","    #myfunc_model2(ordinary_tweets_data_mask)\n","    return model_1_result2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuFnbsOCPVx2","executionInfo":{"status":"ok","timestamp":1669349873209,"user_tz":-330,"elapsed":387,"user":{"displayName":"Nadeeka Kiringoda","userId":"02230049152691302065"}},"outputId":"3c314dab-94e1-4fc9-80cb-78df462f906c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# MODEL TWO"],"metadata":{"id":"qpROeSDbc-jp"}},{"cell_type":"code","source":["def load_credible_data():\n","    data = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/credible_tweets.csv\")\n","    data.columns = data.columns.str.replace(' ','_')\n","    return data\n","\n","\n","import re,math\n","from collections import Counter\n","\n","WORD = re.compile(r'\\w+')\n","\n"," \n","\n","def model2(dataframe):\n"," #!pip install fasttext\n"," from google.colab import drive\n"," drive.mount('/content/gdrive', force_remount=True)\n"," root_path = 'gdrive/My Drive/Colab Notebooks/'\n"," from scipy.spatial.distance import cosine\n"," import numpy as np\n"," import fasttext\n"," import fasttext.util\n"," #!gunzip \"/content/gdrive/My Drive/Colab Notebooks/cc.en.300.bin.gz\" -d \"/content/gdrive/My Drive/Colab Notebooks/cc.en.300.bin\"\n"," model = fasttext.load_model('/content/gdrive/My Drive/Colab Notebooks/cc.en.300.bin')\n"," def text_to_vector(text):\n","   words = model.get_sentence_vector(text)\n","   # WORD.findall()\n","   return words\n","\n"," def text_to_vector1(text):\n","    text = [model.get_sentence_vector(word) for word in text]\n","    return text\n","\n"," def text_to_vector2(text):\n","    text = [model.get_word_vector(word) for word in text]\n","    return text\n","\n","\n"," def text_to_vector3(text):\n","    text = model.get_word_vector(text)\n","    return text   \n"," tweet_credible_df = load_credible_data()\n","\n"," ordinary_tweets_data = pd.read_csv(dataframe)\n"," ordinary_tweets_data.columns = ordinary_tweets_data.columns.str.replace(' ','_')\n"," pattern='Colombo,Sri Lanka |Colombo, Sri Lanka|Sri Lanka| Srilanka ðŸ‡±ðŸ‡° |Colombo|Colombo, Srilanka|Colombo Sri Lanka|Sri Lankan'\n"," mask=ordinary_tweets_data['Tweet_Location'].str.contains(pattern,case=False,na=False)\n"," ordinary_tweets_data_mask=ordinary_tweets_data[mask]\n","\n","\n"," df  = pd.DataFrame(ordinary_tweets_data_mask[['Tweet_Id','Tweet_Type','User_Id','Name','Screen_Name', 'Tweet_Content','Retweets_Received','Favourites_Received',\n","                            'Verified_or_Non-Verified','User_Followers','User_Following','Favourites_Count','Statuses_Count',\n","                             'User_Account_Creation_Date','User_Bio','Tweet_Location','Tweet_Posted_Time_(UTC)','Protected_or_Non-protected']])\n","\n"," df_credible  = pd.DataFrame(tweet_credible_df[['Tweet_Id','Tweet_Type','User_Id','Name','Screen_Name', 'Tweet_Content','Retweets_Received','Favourites_Received',\n","                            'Verified_or_Non-Verified','User_Followers','User_Following','Favourites_Count','Statuses_Count',\n","                             'User_Account_Creation_Date','User_Bio','Tweet_Location','Tweet_Posted_Time_(UTC)','Protected_or_Non-protected']])\n"," df[\"Tweet_Content_URL\"] = df[\"Tweet_Content\"].str.replace(r'\\s*https?://\\S+(\\s+|$)', ' ').str.strip()\n"," df['Tweet_punct'] = df['Tweet_Content_URL'].apply(lambda x: remove_punct(x))\n"," df['User_Id_punct'] = df['User_Id'].apply(lambda x: x.replace('\"', ''))\n"," df['Tweet_Id_punct'] = df['Tweet_Id'].apply(lambda x: x.replace('\"', ''))\n"," df['Tweet_tokenized'] = df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\n"," df['Tweet_nonstop'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n"," df['Tweet_stemmed'] = df['Tweet_nonstop'].apply(lambda x: stemming(x))\n"," df['Tweet_lemmatized'] = df['Tweet_nonstop'].apply(lambda x: lemmatizer(x))\n","\n","#cleaning the credible datasource \n","\n"," df_credible[\"Tweet_Content_URL\"] = df_credible[\"Tweet_Content\"].str.replace(r'\\s*https?://\\S+(\\s+|$)', ' ').str.strip()\n"," df_credible['Tweet_punct'] = df_credible['Tweet_Content_URL'].apply(lambda x: remove_punct(x))\n"," df_credible['User_Id_punct'] = df_credible['User_Id'].apply(lambda x: x.replace('\"', ''))\n"," df_credible['Tweet_Id_punct'] = df_credible['Tweet_Id'].apply(lambda x: x.replace('\"', ''))\n"," df_credible['Tweet_tokenized'] = df_credible['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\n"," df_credible['Tweet_nonstop'] = df_credible['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n"," df_credible['Tweet_stemmed'] = df_credible['Tweet_nonstop'].apply(lambda x: stemming(x))\n"," df_credible['Tweet_lemmatized'] = df_credible['Tweet_nonstop'].apply(lambda x: lemmatizer(x))\n","\n"," model_2_dataset1  = pd.DataFrame(df[['User_Id_punct','Tweet_Id_punct', 'Tweet_lemmatized']])\n"," model_2_dataset1.rename(columns = {'Tweet_lemmatized':'Tweet_Content','User_Id_punct':'User_Id','Tweet_Id_punct':'Tweet_Id'}, inplace = True)\n","\n"," df_credible_apply_model2=pd.DataFrame(df_credible[['User_Id_punct','Tweet_Id_punct', 'Tweet_lemmatized']])\n"," df_credible_apply_model2.rename(columns = {'Tweet_lemmatized':'Tweet_Content','User_Id_punct':'User_Id','Tweet_Id_punct':'Tweet_Id'}, inplace = True)\n","\n","\n"," #return model_1_result2\n","\n"," model_2_dataset1['Tweet_untokanized']= model_2_dataset1['Tweet_Content'].apply(lambda x: untokenize(x))\n"," model_2_dataset2  = pd.DataFrame(model_2_dataset1[['User_Id','Tweet_Id' ,'Tweet_untokanized']])\n"," model_2_dataset2.rename(columns = {'Tweet_untokanized':'Tweet_Content'}, inplace = True)\n","\n","\n","##################################################model 2 credible details\n","\n"," df_credible_apply_model2['Tweet_untokanized']= df_credible_apply_model2['Tweet_Content'].apply(lambda x: untokenize(x))\n"," df_credible_apply_model2  = pd.DataFrame(df_credible_apply_model2[['User_Id','Tweet_Id', 'Tweet_untokanized']])\n"," df_credible_apply_model2.rename(columns = {'Tweet_untokanized':'Tweet_Content'}, inplace = True)\n","\n","\n"," model_2_dataset2['Tweet_sentence_vec'] = model_2_dataset2['Tweet_Content'].apply(lambda x: text_to_vector(x))\n"," model_2_dataset2['Tweet_word_vec'] = model_2_dataset2['Tweet_Content'].apply(lambda x: text_to_vector3(x))\n","##################################sentence and word vecotr for credible sources\n","\n"," df_credible_apply_model2['Tweet_sentence_vec'] = df_credible_apply_model2['Tweet_Content'].apply(lambda x: text_to_vector(x))\n"," df_credible_apply_model2['Tweet_word_vec'] = df_credible_apply_model2['Tweet_Content'].apply(lambda x: text_to_vector3(x))\n","\n","#Tweet_sentence_vec\n","\n"," from sklearn.metrics.pairwise import cosine_similarity\n","\n"," s = df_credible_apply_model2['Tweet_sentence_vec'].sample(n=1).tolist()\n","\n"," model_2_dataset2['Tweet_sentence_com'] = np.tile(s, (len(model_2_dataset2), 1)).tolist()\n"," model_2_dataset2['cosine_sim_senetence'] = cosine_similarity(model_2_dataset2['Tweet_sentence_vec'].tolist(), s)\n","\n","\n"," s1 = df_credible_apply_model2['Tweet_word_vec'].sample(n=1).tolist()\n","\n"," model_2_dataset2['Tweet_word_vec_com'] = np.tile(s, (len(model_2_dataset2), 1)).tolist()\n"," model_2_dataset2['cosine_sim_word'] = cosine_similarity(model_2_dataset2['Tweet_word_vec'].tolist(), s)\n"," return model_2_dataset2.head(20)"],"metadata":{"id":"of-NRHmKc91T","executionInfo":{"status":"ok","timestamp":1669349891676,"user_tz":-330,"elapsed":1532,"user":{"displayName":"Nadeeka Kiringoda","userId":"02230049152691302065"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# MODEL THREE"],"metadata":{"id":"SQsHJPRZ-ZGn"}},{"cell_type":"code","source":["def calculation_trust_score(x):\n","   return x* 0.5\n","\n","def labelling(value):\n","    if value > 0.5:\n","      return 'Real'\n","    return 'Fake'\n","\n","def model3(CSV_PATH):\n","  CSV_PATH_MODEL1=CSV_PATH\n","  MODEL_1=model1(CSV_PATH_MODEL1)\n","  model_2=model2(CSV_PATH_MODEL1)\n","  \n","  #df3 = pd.concat([MODEL_1, model_2],axis=1, ignore_index=False, sort=False)\n","  MODEL_1['User_Id']=MODEL_1['User_Id'].astype(str).str[:50]\n","\n","  df3=pd.merge(MODEL_1,model_2, on='User_Id')\n","\n","  Final_dataset  = pd.DataFrame(df3[['User_Id','Tweet_Id','Tweet_Content','normalized_final_weighted_score','cosine_sim_senetence','cosine_sim_word']])\n","\n","  #Final_dataset = Final_dataset.loc[:, ~Final_dataset.columns.duplicated()]\n","\n","  Final_dataset[['trust_score_model1','trust_score_model2_sentence','trust_score_model2_word']] = Final_dataset[['normalized_final_weighted_score','cosine_sim_senetence','cosine_sim_word']].apply(calculation_trust_score)\n","\n","  Final_dataset= Final_dataset.eval('Sentenct_level_trust_score = trust_score_model2_sentence + trust_score_model1')\n","  Final_dataset= Final_dataset.eval('word_level_trust_score = trust_score_model2_word + trust_score_model1')\n","\n","\n","  Final_dataset['Label_senetence_level'] = Final_dataset['Sentenct_level_trust_score'].apply(labelling)\n","  Final_dataset['Label_word_level'] = Final_dataset['word_level_trust_score'].apply(labelling)\n","  Final_dataset.rename(columns = {'Label_senetence_level':'Label(FAKE/REAL)'}, inplace = True)\n","  Final_LABEL=pd.DataFrame(Final_dataset[['Tweet_Id','Label(FAKE/REAL)']])\n","  return Final_LABEL\n","\n"],"metadata":{"id":"zQSyFtQ65FuG","executionInfo":{"status":"ok","timestamp":1669349900957,"user_tz":-330,"elapsed":834,"user":{"displayName":"Nadeeka Kiringoda","userId":"02230049152691302065"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# CALLING TO THE MODEL"],"metadata":{"id":"9ID9NclwWiUE"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/Colab Notebooks/'\n","#/content/gdrive/MyDrive/Colab Notebooks/srilanka_easter.csv\n","model3('/content/gdrive/MyDrive/Colab Notebooks/srilanka_easter.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":764},"id":"x_9_-KsPQJYJ","executionInfo":{"status":"ok","timestamp":1669354716964,"user_tz":-330,"elapsed":77048,"user":{"displayName":"Nadeeka Kiringoda","userId":"02230049152691302065"}},"outputId":"e3b45b0c-72a3-4bcb-92c3-98874c40e5cc"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Mounted at /content/gdrive\n"]},{"output_type":"stream","name":"stderr","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: FutureWarning: The default value of regex will change from True to False in a future version.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: FutureWarning: The default value of regex will change from True to False in a future version.\n"]},{"output_type":"execute_result","data":{"text/plain":["               Tweet_Id Label(FAKE/REAL)\n","0   1130097107333836800             Real\n","1   1129374574527897601             Real\n","2   1130043387724353536             Real\n","3   1129755645698297857             Real\n","4   1129449756512727040             Real\n","5   1129435664393691138             Real\n","6   1129369858779566081             Real\n","7   1128314148251738113             Real\n","8   1128310201520676864             Real\n","9   1128309202299052032             Real\n","10  1128308434783408128             Real\n","11  1129382522335375366             Real\n","12  1129298331744768000             Real\n","13  1128966069069258753             Real\n","14  1129290475507191808             Real\n","15  1128982500901117952             Real\n","16  1128950288507760640             Real\n","17  1128885974929432579             Real\n","18  1128879152457641985             Real\n","19  1128532622915907584             Real"],"text/html":["\n","  <div id=\"df-f5913cad-b0fb-41eb-988f-ba1a486fd6a8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet_Id</th>\n","      <th>Label(FAKE/REAL)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1130097107333836800</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1129374574527897601</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1130043387724353536</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1129755645698297857</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1129449756512727040</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1129435664393691138</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1129369858779566081</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1128314148251738113</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1128310201520676864</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1128309202299052032</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1128308434783408128</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1129382522335375366</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1129298331744768000</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1128966069069258753</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1129290475507191808</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1128982500901117952</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1128950288507760640</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1128885974929432579</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1128879152457641985</td>\n","      <td>Real</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1128532622915907584</td>\n","      <td>Real</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5913cad-b0fb-41eb-988f-ba1a486fd6a8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f5913cad-b0fb-41eb-988f-ba1a486fd6a8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f5913cad-b0fb-41eb-988f-ba1a486fd6a8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["# Model deployment"],"metadata":{"id":"GCo4X2YMot_0"}},{"cell_type":"code","source":["@anvil.server.callable\n","def model3(text_box_1):\n","  \n","  return model3(text_box_1)\n"],"metadata":{"id":"LMkeHwAbWjFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DEMI5TFZE7ACXMV35LDUH7XQ-BO2AFU2NYQE3OIDQ\n","!pip install anvil-uplink\n","import pandas as pd\n","import numpy as np\n","\n","from anvil.tables import app_tables\n","import anvil.server\n","import anvil.media\n","\n","anvil.server.connect(\"DEMI5TFZE7ACXMV35LDUH7XQ-BO2AFU2NYQE3OIDQ\") \n","\n","def _upload_dataframe(df, rows):\n","    df = df.replace({np.nan: None})\n","    dicts = df.to_dict(orient=\"records\")\n","    if rows:\n","        dicts = list(dicts)[:rows]\n","    if len(dicts) < 200:\n","        # This way is faster:\n","        anvil.server.call('save_to_table', dicts)\n","    else:\n","        # But keep large file processing here to avoid server timeouts:\n","        for d in dicts:\n","            app_tables.my_table.add_row(**d)\n","\n","@anvil.server.callable\n","def upload_csv_data(file, rows=None):\n","    with anvil.media.TempFile(file) as f:\n","        df = pd.read_csv(f)\n","        _upload_dataframe(df, rows)\n","\n","@anvil.server.callable\n","def upload_excel_data(file, rows=None):\n","    with anvil.media.TempFile(file) as f:\n","        df = pd.read_excel(f)\n","        _upload_dataframe(df, rows)\n","\n","anvil.server.wait_forever()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"jYYlugbCV0cH","executionInfo":{"status":"error","timestamp":1669309946622,"user_tz":-330,"elapsed":100659,"user":{"displayName":"Nadeeka Kiringoda","userId":"02230049152691302065"}},"outputId":"c20ac256-4804-4c50-8889-b434e0221954"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-f44d06768567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#DEMI5TFZE7ACXMV35LDUH7XQ-BO2AFU2NYQE3OIDQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0manvil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/anvil/server.py\u001b[0m in \u001b[0;36mwait_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}